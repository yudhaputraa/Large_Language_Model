{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# melihat perangkat GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "blok_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs : \n",
      "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [33,  1, 66, 54, 64, 58,  1, 78],\n",
      "        [ 1, 57, 62, 72, 73, 54, 67, 56],\n",
      "        [61, 58,  1, 69, 54, 72, 72, 54]], device='cuda:0')\n",
      "targets : \n",
      "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 1, 66, 54, 64, 58,  1, 78, 68],\n",
      "        [57, 62, 72, 73, 54, 67, 56, 58],\n",
      "        [58,  1, 69, 54, 72, 72, 54, 60]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_bacth(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - blok_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    x = torch.stack([data[i:i+blok_size] for i in ix])\n",
    "    #print(x)\n",
    "    y = torch.stack([data[i+1:i+blok_size+1] for i in ix])\n",
    "    #print(y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_bacth('train')\n",
    "print('inputs : ')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets : ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_bacth(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@torch.no_grad()` adalah suatu konteks (context manager) yang digunakan untuk menonaktifkan perhitungan gradien selama eksekusi blok tertentu. Ini berguna ketika kita hanya ingin melakukan inferensi atau evaluasi model tanpa memperbarui bobot (weights) dan membutuhkan penyimpanan gradien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([80]) target is tensor(1)\n",
      "when input is tensor([80,  1]) target is tensor(1)\n",
      "when input is tensor([80,  1,  1]) target is tensor(28)\n",
      "when input is tensor([80,  1,  1, 28]) target is tensor(39)\n",
      "when input is tensor([80,  1,  1, 28, 39]) target is tensor(42)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42]) target is tensor(39)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39]) target is tensor(44)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) target is tensor(32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x = train_data[:blok_size]\n",
    "y = train_data[1:blok_size+1]\n",
    "\n",
    "for t in range(blok_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('when input is',context, 'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BigramLanguageModel(nn.Module):\n",
    "#     def __init__(self, vocab_size):\n",
    "#         super().__init__()\n",
    "#         self.token_embeddings_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "#     def forward(self, index, targets):\n",
    "#         logits = self.token_embeddings_table(index)\n",
    "#         B,T,C = logits.shape\n",
    "#         logits = logits.view(B*T,C)\n",
    "#         targets = targets.view(B*T)\n",
    "#         loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b)TVQJEF﻿7\"gbV,6dz[?gG0\n",
      "lct )?r;aD0tzw-9oh;i.o*pjMqolJ2'J?PMuR4vOIvw(3SV_7PfBvhh6d;r!pNp-1UpCTA4cm[](P?024\"Qwz:!6himUjE*a5VGVcmsZ;tI\"VEvRR_,A\n",
      "p-ySZ.7''G\"Cup-a)\n",
      "p-﻿,JIvgiFt&4VYTPa)._CEOA;4rXLjxT8kLS-40&aDkqpa4Bh6-S(&Lj[eq*H37R5a﻿9jF﻿[&4S)Yd--7o36 7vWvku;dX-(ci-g,6R)c[(8I\n",
      "lKu30n!7hhA5uq8b2,c&﻿'0LUBht!ITTA.aq'IvRbc7]blyksQfIV:\"V7gHGp??'Q[3saMz2]SP4B-gbe!5w9D3.TM3HrIvC30Iwot2r309K?0tkfSrgdGzEfM.H1?IPZ_lDg2?H !QN!p3JsQ)HQOv3*a-FV?nIJ.8.BT﻿hj30t(C]o1G9ppuOCvC3JneIJFxIK!k)!k9S6jzFQ&9DqdLaB;QOI,2cs5g]u.\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # fokus only on the last time step\n",
    "            logits = logits[:, -1, :] # become (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append samples index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Embedding()` adalah modul embedding layer. Embedding layer digunakan untuk mengimplementasikan representasi vektor spasial untuk token atau indeks kategori diskrit tertentu. Representasi ini disebut embedding, dan itu memungkinkan model untuk \"mempelajari\" representasi terbaik untuk setiap token atau kategori selama proses pelatihan.\n",
    "\n",
    "`F.cross_entropy()` pada PyTorch adalah fungsi loss yang sering digunakan dalam tugas klasifikasi multikelas. Fungsi ini menggabungkan operasi log softmax dan negative log likelihood loss (NLLLoss) dalam satu langkah efisien.\n",
    "- Log Softmax: Menerapkan operasi log softmax pada setiap prediksi, mengubah nilai-nilai prediksi menjadi skor probabilitas yang sesuai.\n",
    "- Negative Log Likelihood Loss (NLLLoss): Menghitung loss sebagai negatif log likelihood dari kelas yang benar. Ini memberikan bobot yang lebih besar pada kesalahan kelas yang serius.\n",
    "\n",
    "`F.softmax()` adalah fungsi yang digunakan untuk mengaplikasikan operasi softmax pada tensor. Softmax mengonversi nilai-nilai dalam tensor menjadi distribusi probabilitas sehingga jumlah semua probabilitasnya menjadi 1.\n",
    "\n",
    "Softmax adalah suatu fungsi yang digunakan untuk mengonversi keluaran dari suatu model atau sistem ke dalam bentuk distribusi probabilitas. Fungsi ini digunakan khususnya dalam konteks klasifikasi, di mana model menghasilkan skor atau logit untuk setiap kelas, dan softmax kemudian mengubah skor tersebut menjadi probabilitas yang dapat diinterpretasikan.\n",
    "\n",
    "`torch.multinomial()` adalah fungsi yang digunakan untuk mengambil sampel indeks dari distribusi multinomial. Fungsi ini sering digunakan dalam konteks pembelajaran mesin untuk memperoleh sampel berdasarkan distribusi probabilitas yang diberikan.\n",
    "\n",
    "Fungsi ini sering digunakan dalam berbagai aplikasi, seperti saat mengambil sampel kata dari distribusi probabilitas dalam model bahasa, atau saat mengimplementasikan metode Markov Chain Monte Carlo (MCMC).\n",
    "\n",
    "`torch.cat()` adalah fungsi yang digunakan untuk menggabungkan (concatenate) tensor di sepanjang suatu dimensi tertentu. Fungsi ini memungkinkan kita menggabungkan beberapa tensor menjadi satu tensor.\n",
    "\n",
    "- Argumen: Argumen pertama adalah tuple dari tensor-tensor yang ingin digabungkan. Argument kedua, dim, menunjukkan dimensi di sepanjang mana penggabungan akan dilakukan.\n",
    "- Dimensi: Dimensi yang digunakan untuk penggabungan harus memiliki ukuran yang sama, kecuali untuk dimensi yang digabungkan. Misalnya, jika menggabungkan tensor 2D sepanjang dimensi 0, jumlah kolom harus sama.\n",
    "- Penggabungan Banyak Tensor: torch.cat() dapat digunakan untuk menggabungkan lebih dari dua tensor sekaligus dengan menyediakan tuple yang sesuai dengan jumlah tensor yang ingin digabungkan.\n",
    "\n",
    "Fungsi ini sangat berguna dalam berbagai situasi, terutama ketika kita bekerja dengan data yang perlu digabungkan secara fleksibel, seperti dalam pengolahan gambar atau dalam pembentukan dataset untuk model pembelajaran mesin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0: train loss : 3.2113, val loss : 3.2183\n",
      "step : 250: train loss : 3.1657, val loss : 3.1971\n",
      "step : 500: train loss : 3.1291, val loss : 3.1565\n",
      "step : 750: train loss : 3.1181, val loss : 3.1402\n",
      "2.9627227783203125\n"
     ]
    }
   ],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step : {iter}: train loss : {losses['train']:.4f}, val loss : {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_bacth('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*need to familiarize audience with optimizers (AdamW, Adam, SGD, MSE…) no need to jump into the formulas, just what the optimizer does for us and some of the differences/similarities between them*\n",
    "\n",
    "1. Mean Squared Error (MSE): MSE is a common loss function used in regression problems, where the goal is to predict a continuous output. It measures the average squared difference between the predicted and actual values, and is often used to train neural networks for regression tasks.\n",
    "2. Gradient Descent (GD): is an optimization algorithm used to minimize the loss function of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. The idea of GD is to iteratively adjust the model parameters in the direction of the steepest descent of the loss function\n",
    "3. Momentum: Momentum is an extension of SGD that adds a \"momentum\" term to the parameter updates. This term helps smooth out the updates and allows the optimizer to continue moving in the right direction, even if the gradient changes direction or varies in magnitude. Momentum is particularly useful for training deep neural networks.\n",
    "4. RMSprop: RMSprop is an optimization algorithm that uses a moving average of the squared gradient to adapt the learning rate of each parameter. This helps to avoid oscillations in the parameter updates and can improve convergence in some cases.\n",
    "5. Adam: Adam is a popular optimization algorithm that combines the ideas of momentum and RMSprop. It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. Adam is often used as a default optimizer for deep learning models.\n",
    "6. AdamW: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. This helps to regularize the model and can improve generalization performance. We will be using the AdamW optimizer as it best suits the properties of the model we will train in this video.\n",
    "\n",
    "find more optimizers and details at torch.optim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
